{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_processing analysis .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LqvvhGx-8Q3"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "ps = PorterStemmer()\n",
        "ls = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "294wWae0_HM6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frcHi_lI-8RA",
        "outputId": "fd7145bf-8ecd-4d99-a3df-847db8b67c17"
      },
      "source": [
        "df = pd.read_csv(\"Dataset.csv\")              \n",
        "print(df.shape)\n",
        "tweets = df['text']\n",
        "print(tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(211746, 12)\n",
            "0         How has #PTSD affected this writer's #quaranti...\n",
            "1          #blackandyellow #Love #moments #september #Au...\n",
            "2         Things I Do In My Spare Time Play Game 1- Eco ...\n",
            "3         每天花5-30分钟进行正念冥想。使用此链接可帮助您开始冥想：https://www.apa....\n",
            "4         Your #Quarantine #MovieList: THE HOUSE ON SORO...\n",
            "                                ...                        \n",
            "211741    A man in Gujranwala* got his ass kicked by his...\n",
            "211742    @joobsie Astaga! Dua kali aku kena slap malam ...\n",
            "211743    RT @sfrizwan: #ViolenceAgainstMen RT @AmazingS...\n",
            "211744    #ViolenceAgainstMen RT @AmazingSusan Women rap...\n",
            "211745    Spotlight On Male Victims Of Domestic Abuse ht...\n",
            "Name: text, Length: 211746, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvvNdgFA-8RN"
      },
      "source": [
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn5kKupP-8RV",
        "outputId": "55904362-9927-48bb-d4ed-6aeb4e44f918"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def cleaning(text):\n",
        "    text = decontracted(text)\n",
        "    text = text.lower()                              #lowering the text\n",
        "    text = re.sub(r'@\\S+','',text)                   #Removed @mentions\n",
        "    text = re.sub(r'#\\S+','',text)                   #Remove the hyper link\n",
        "    text = re.sub(r'RT\\S+','',text)                  #Removing ReTweets\n",
        "    text = re.sub(r'https?\\S+','',text)              #Remove the hyper link\n",
        "    text = re.sub('[^a-z]',' ',text)              #Remove the character other than alphabet\n",
        "    text = text.split()\n",
        "    return text\n",
        "tweets_cleaned =  tweets.apply(cleaning)\n",
        "tweets_cleaned"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         [how, has, affected, this, writer, is, experie...\n",
              "1                                                        []\n",
              "2         [things, i, do, in, my, spare, time, play, gam...\n",
              "3                                                        []\n",
              "4         [your, the, house, on, sorority, row, my, review]\n",
              "                                ...                        \n",
              "211741    [a, man, in, gujranwala, got, his, ass, kicked...\n",
              "211742    [astaga, dua, kali, aku, kena, slap, malam, ni...\n",
              "211743                       [rt, rt, women, rape, men, in]\n",
              "211744                           [rt, women, rape, men, in]\n",
              "211745    [spotlight, on, male, victims, of, domestic, a...\n",
              "Name: text, Length: 211746, dtype: object"
            ]
          },
          "execution_count": 23,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIRICSWS-8Rb"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "tweets_stemmed = tweets_cleaned.apply(lambda x: [ps.stem(word) for word in x if word not in stop_words])\n",
        "tweets_lemmatized = tweets_cleaned.apply(lambda x: [ls.lemmatize(word) for word in x if word not in stop_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8JzEP2c-8Rg"
      },
      "source": [
        "tweets_stemmed = tweets_stemmed.apply(lambda x: ' '.join(x))\n",
        "tweets_lemmatized = tweets_lemmatized.apply(lambda x: ' '.join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MBpgzrW-8Rn"
      },
      "source": [
        "df['scrapped_text'] = df['text']\n",
        "df['Lemmatized_text'] = tweets_lemmatized.to_frame() \n",
        "df['Stemmed_text'] = tweets_lemmatized.to_frame()\n",
        "new_df=df.drop(['Unnamed: 0', 'id', 'permalink', 'username', 'to', 'text', 'date','retweets', 'favorites', 'mentions', 'hashtags', 'geo',],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee8Ultrn-8Ru",
        "outputId": "e3f38654-cd4a-4dd3-f14b-b54dcf223445"
      },
      "source": [
        "new_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>scrapped_text</th>\n",
              "      <th>Lemmatized_text</th>\n",
              "      <th>Stemmed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How has #PTSD affected this writer's #quaranti...</td>\n",
              "      <td>affected writer experience click link find</td>\n",
              "      <td>affected writer experience click link find</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#blackandyellow #Love #moments #september #Au...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Things I Do In My Spare Time Play Game 1- Eco ...</td>\n",
              "      <td>thing spare time play game eco unisex tee usd ...</td>\n",
              "      <td>thing spare time play game eco unisex tee usd ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>每天花5-30分钟进行正念冥想。使用此链接可帮助您开始冥想：https://www.apa....</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Your #Quarantine #MovieList: THE HOUSE ON SORO...</td>\n",
              "      <td>house sorority row review</td>\n",
              "      <td>house sorority row review</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       scrapped_text  \\\n",
              "0  How has #PTSD affected this writer's #quaranti...   \n",
              "1   #blackandyellow #Love #moments #september #Au...   \n",
              "2  Things I Do In My Spare Time Play Game 1- Eco ...   \n",
              "3  每天花5-30分钟进行正念冥想。使用此链接可帮助您开始冥想：https://www.apa....   \n",
              "4  Your #Quarantine #MovieList: THE HOUSE ON SORO...   \n",
              "\n",
              "                                     Lemmatized_text  \\\n",
              "0         affected writer experience click link find   \n",
              "1                                                      \n",
              "2  thing spare time play game eco unisex tee usd ...   \n",
              "3                                                      \n",
              "4                          house sorority row review   \n",
              "\n",
              "                                        Stemmed_text  \n",
              "0         affected writer experience click link find  \n",
              "1                                                     \n",
              "2  thing spare time play game eco unisex tee usd ...  \n",
              "3                                                     \n",
              "4                          house sorority row review  "
            ]
          },
          "execution_count": 27,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQvK0GRb-8R0"
      },
      "source": [
        "new_df.to_csv('New_Dataset_3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUnCQY7U-8R-"
      },
      "source": [
        "## Model Building Starts From here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdLewv-FDqDU"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "ps = PorterStemmer()\n",
        "ls = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNXT5YBy-8R_"
      },
      "source": [
        "df=pd.read_csv('New_Dataset_3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqgoHfrO-8SD"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7b7zbcx-8SO"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwWna0Yq-8SV"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKxYc0zV-8Sd"
      },
      "source": [
        "df=df.dropna(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObRNhiN1-8Sh"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0SVIszU-8Sm"
      },
      "source": [
        "## Creating Stemmed AND Lemmatized text Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir8nOVl5-8Sn"
      },
      "source": [
        "stem_df=df[['Stemmed_text']]\n",
        "lemm_df=df[['Lemmatized_text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2esmu_7-8Su"
      },
      "source": [
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid=SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DnN2hbi-8Sz"
      },
      "source": [
        "stem_df['scores']=stem_df['Stemmed_text'].apply(lambda Stemmed_text :sid.polarity_scores(Stemmed_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWfb1Yd3-8S4"
      },
      "source": [
        "stem_df['compound']=stem_df['scores'].apply(lambda score_dict:score_dict['compound'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7jqIYgx-8TA"
      },
      "source": [
        "stem_df['comp_score']=stem_df['compound'].apply(lambda c:'pos' if c>=0 else \"neg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ3sLmnS-8TH"
      },
      "source": [
        "stem_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atdvb8XRjq6O"
      },
      "source": [
        "stem_df['comp_score'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2GkNpO8-8TO"
      },
      "source": [
        "## Try to play with this number and check how much it work in bag of words \n",
        "stem_df=stem_df.iloc[:20000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7A07yCn-8TV"
      },
      "source": [
        "stem_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJnaDBG5-8Ta"
      },
      "source": [
        "## Implementing Bag of Words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxkRBujr-8Tb"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer()\n",
        "\n",
        "X=cv.fit_transform(stem_df['Stemmed_text']).toarray()\n",
        "Y=pd.get_dummies(stem_df['comp_score'])\n",
        "Y=Y.iloc[:,1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBRKfIjVi6IO"
      },
      "source": [
        "Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u0m7a6N-8Th"
      },
      "source": [
        "X.shape,Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSlRAOEw-8Ts"
      },
      "source": [
        "## Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idfZB_20-8Tt"
      },
      "source": [
        "from sklearn.model_selection  import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRw3Vd4--8T1"
      },
      "source": [
        "## Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__HSmy-S-8T1"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model=MultinomialNB().fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGRYBn97B2_2"
      },
      "source": [
        "y_pred=model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPgLGObvEVGz"
      },
      "source": [
        "## Finding accuracy_score and printing confusion_matrix and classification_report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY-cxQs2D89Y"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
        "accuracy_score(y_test,y_pred)  #Testing Score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JzhpRb7lFD5"
      },
      "source": [
        "accuracy_score(y_train, model.predict(x_train))  #Training Score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dne9knFHEdm_"
      },
      "source": [
        "confusion_matrix(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd43MgwXEeSp"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msDd8pz0Ese9"
      },
      "source": [
        "## Predicting a new tweet sentiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aM7d7i7Ei90"
      },
      "source": [
        "df=pd.DataFrame([\"i am tweeting because there is domestic violence spreading everywhere\"],columns=['new_tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buX8thA3FLl3"
      },
      "source": [
        "df   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RcqZ6lvFfGy"
      },
      "source": [
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "nltk.download('stopwords')\n",
        "def cleaning(text):\n",
        "    corpus = []\n",
        "    text = decontracted(text)\n",
        "    text = text.lower()                              #lowering the text\n",
        "    text = re.sub(r'@\\S+','',text)                   #Removed @mentions\n",
        "    text = re.sub(r'#\\S+','',text)                   #Remove the hyper link\n",
        "    text = re.sub(r'RT\\S+','',text)                  #Removing ReTweets\n",
        "    text = re.sub(r'https?\\S+','',text)              #Remove the hyper link\n",
        "    text = re.sub('[^a-z]',' ',text)              #Remove the character other than alphabet\n",
        "    text = text.split()\n",
        "    text=[ps.stem(word) for word in text if word not in stopwords.words('english')]\n",
        "    text=' '.join(text)\n",
        "    corpus.append(text)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-6o3YcpFNoH"
      },
      "source": [
        "tweet=df['new_tweet']\n",
        "tweet_cleaned =  cleaning(tweet[0])\n",
        "type(tweet_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JAC6Y-9FyK8"
      },
      "source": [
        "df=cv.transform(tweet_cleaned).toarray()\n",
        "\n",
        "pred=model.predict(df)\n",
        "\n",
        "label=pred[0]\n",
        "\n",
        "if label==1:\n",
        "  print('Positive')\n",
        "else:\n",
        "  print('Negative')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVH0rLYa7QHK"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0or_0BFpGY4m"
      },
      "source": [
        "from pickle import dump\n",
        "\n",
        "dump(cv,open('pickle/countvectorizer.pkl','wb'))\n",
        "\n",
        "dump(model,open('pickle/model.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShqGNUWdo_HN"
      },
      "source": [
        "## Predicting new tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z102wcMU7QHM"
      },
      "source": [
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "nltk.download('stopwords')\n",
        "def cleaning(text):\n",
        "    corpus = []\n",
        "    text = decontracted(text)\n",
        "    text = text.lower()                              #lowering the text\n",
        "    text = re.sub(r'@\\S+','',text)                   #Removed @mentions\n",
        "    text = re.sub(r'#\\S+','',text)                   #Remove the hyper link\n",
        "    text = re.sub(r'RT\\S+','',text)                  #Removing ReTweets\n",
        "    text = re.sub(r'https?\\S+','',text)              #Remove the hyper link\n",
        "    text = re.sub('[^a-z]',' ',text)              #Remove the character other than alphabet\n",
        "    text = text.split()\n",
        "    text=[ps.stem(word) for word in text if word not in stopwords.words('english')]\n",
        "    text=' '.join(text)\n",
        "    corpus.append(text)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-aOLkYy7QHN"
      },
      "source": [
        "from pickle import load\n",
        "\n",
        "def predict(input_msg):\n",
        "    vectorizer = load(open('pickle/countvectorizer.pkl','rb'))\n",
        "    \n",
        "    classifier = load(open('pickle/model.pkl','rb'))\n",
        "    \n",
        "    clean_text = cleaning(input_msg)\n",
        "    \n",
        "    clean_text_encoded = vectorizer.transform(clean_text)\n",
        "    \n",
        "    future_text = clean_text_encoded.toarray()\n",
        "    \n",
        "    prediction = classifier.predict(future_text)\n",
        "    \n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTpE6y0S7QHO"
      },
      "source": [
        "input_tweet = input()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Fai0it7QHO"
      },
      "source": [
        "prediction = predict(input_tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKxx_fTU7QHP",
        "outputId": "3577c982-688a-47d5-c14a-da1a9e993612"
      },
      "source": [
        "if prediction == 1:\n",
        "  print('Positive')\n",
        "else:\n",
        "  print('Negative')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive\n"
          ]
        }
      ]
    }
  ]
}